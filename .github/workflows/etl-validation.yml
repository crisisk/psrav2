name: ETL Pipeline Validation

on:
  pull_request:
    paths:
      - "psr/**"
      - "backend/app/etl/**"
      - "backend/app/dal/**"
      - "backend/app/contracts/**"
      - "backend/app/db/**"
      - "pyproject.toml"
      - "poetry.lock"
  push:
    branches: [main]
    paths:
      - "psr/**"
      - "backend/app/etl/**"
      - "backend/app/dal/**"
      - "backend/app/contracts/**"
      - "backend/app/db/**"
      - "pyproject.toml"
      - "poetry.lock"
  workflow_dispatch:

jobs:
  validate-etl:
    name: Validate ETL Pipeline (Dry-Run)
    runs-on: ubuntu-22.04
    timeout-minutes: 15

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.11"
          cache: "pip"

      - name: Set up Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          components: rustfmt, clippy

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y --no-install-recommends \
            pkg-config \
            libssl-dev \
            build-essential

      - name: Install Poetry
        run: |
          python -m pip install --upgrade pip
          python -m pip install poetry==1.7.1

      - name: Configure Poetry
        run: |
          poetry config virtualenvs.create true
          poetry config virtualenvs.in-project true

      - name: Cache Poetry dependencies
        uses: actions/cache@v4
        with:
          path: .venv
          key: venv-${{ runner.os }}-py3.11-${{ hashFiles('**/poetry.lock') }}
          restore-keys: |
            venv-${{ runner.os }}-py3.11-

      - name: Install Python dependencies
        run: poetry install --with dev --no-interaction

      - name: Validate YAML schema
        run: |
          poetry run python -c "
          from pathlib import Path
          from psr.loader.psr_loader import run_validation, DEFAULT_RULES_DIR, DEFAULT_SCHEMA

          print(f'Running schema validation on {DEFAULT_RULES_DIR}')
          results = run_validation(DEFAULT_SCHEMA, DEFAULT_RULES_DIR)
          failures = [(path, errors) for path, errors in results if errors]

          if failures:
              print(f'❌ Schema validation failed for {len(failures)} file(s):')
              for path, errors in failures:
                  print(f'  - {path}:')
                  for error in errors:
                      print(f'    {error}')
              exit(1)
          else:
              print('✅ All YAML files passed schema validation')
          "

      - name: Run ETL dry-run validation
        run: |
          poetry run python backend/app/etl/ingest_rules.py \
            --validate-only \
            --log-level INFO

      - name: Verify rule count
        run: |
          poetry run python -c "
          from pathlib import Path
          from backend.app.etl.ingest_rules import discover_rule_files, load_rules, DEFAULT_RULES_DIR
          from psr.loader.psr_loader import DEFAULT_RULES_DIR

          rule_files = discover_rule_files(DEFAULT_RULES_DIR)
          print(f'✅ Discovered {len(rule_files)} rule file(s)')

          if len(rule_files) == 0:
              print('❌ No rule files found!')
              exit(1)

          rules = load_rules(rule_files)
          print(f'✅ Successfully parsed {len(rules)} rule(s)')

          # Validate rule IDs are unique
          rule_ids = [r.metadata.rule_id for r in rules]
          if len(rule_ids) != len(set(rule_ids)):
              print('❌ Duplicate rule IDs detected!')
              exit(1)
          print('✅ All rule IDs are unique')
          "

      - name: Validate data quality expectations
        run: |
          poetry run python -c "
          from pathlib import Path
          import pandas as pd
          from great_expectations.dataset import PandasDataset
          from backend.app.etl.ingest_rules import discover_rule_files, load_rules
          from psr.loader.psr_loader import DEFAULT_RULES_DIR

          rule_files = discover_rule_files(DEFAULT_RULES_DIR)
          rules = load_rules(rule_files)

          frame = pd.DataFrame([
              {
                  'rule_id': rule.metadata.rule_id,
                  'agreement_code': rule.metadata.agreement.code,
                  'hs_subheading': rule.metadata.hs_code.subheading,
                  'priority': rule.metadata.priority,
                  'jurisdiction_count': len(rule.metadata.jurisdiction),
              }
              for rule in rules
          ])

          dataset = PandasDataset(frame)

          checks = [
              ('rule_id not null', dataset.expect_column_values_to_not_be_null('rule_id')),
              ('rule_id unique', dataset.expect_column_values_to_be_unique('rule_id')),
              ('agreement_code not null', dataset.expect_column_values_to_not_be_null('agreement_code')),
              ('hs_subheading format', dataset.expect_column_values_to_match_regex('hs_subheading', r'^[0-9]{6}$')),
              ('priority range', dataset.expect_column_values_to_be_between('priority', min_value=0, max_value=999)),
              ('jurisdiction count', dataset.expect_column_values_to_be_between('jurisdiction_count', min_value=1, max_value=10)),
          ]

          all_passed = True
          for name, check in checks:
              status = '✅' if check['success'] else '❌'
              print(f'{status} {name}')
              if not check['success']:
                  all_passed = False

          if not all_passed:
              print('❌ Data quality validation failed!')
              exit(1)
          print('✅ All data quality checks passed')
          "

      - name: Summary
        if: success()
        run: |
          echo "✅ ETL Pipeline Validation Successful"
          echo "- Schema validation: PASSED"
          echo "- Dry-run ingestion: PASSED"
          echo "- Rule uniqueness: PASSED"
          echo "- Data quality: PASSED"
